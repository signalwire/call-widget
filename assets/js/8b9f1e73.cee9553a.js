"use strict";(self.webpackChunkdoc_for_c_2_c_widget=self.webpackChunkdoc_for_c_2_c_widget||[]).push([[120],{5008:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>d});var i=n(4848),s=n(8453);const a={},r="Chat Management",l={id:"developer/Chat",title:"Chat Management",description:"As of yet, the AI-Human chat live transcription feed is sent with a series of events on the",source:"@site/docs/developer/Chat.mdx",sourceDirName:"developer",slug:"/developer/Chat",permalink:"/call-widget/developer/Chat",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"docSidebar",previous:{title:"Development Notes",permalink:"/call-widget/developer/"},next:{title:"Device management",permalink:"/call-widget/developer/Device"}},o={},d=[{value:"When user is talking:",id:"when-user-is-talking",level:2},{value:"When AI is talking uninterrupted:",id:"when-ai-is-talking-uninterrupted",level:2},{value:"When user interrupts (&#39;barge&#39;):",id:"when-user-interrupts-barge",level:2},{value:"handling in code",id:"handling-in-code",level:2},{value:"Deciding who spoke last",id:"deciding-who-spoke-last",level:2},{value:"Code description:",id:"code-description",level:2}];function c(e){const t={code:"code",h1:"h1",h2:"h2",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"chat-management",children:"Chat Management"}),"\n",(0,i.jsx)(t.p,{children:"As of yet, the AI-Human chat live transcription feed is sent with a series of events on the\nmain SignalWire (-> WSClient.on) event."}),"\n",(0,i.jsx)(t.h2,{id:"when-user-is-talking",children:"When user is talking:"}),"\n",(0,i.jsx)(t.mermaid,{value:'stateDiagram-v2\n    "ai.partial_result" --\x3e "ai.partial_result": "Ongoing transcription"\n    "ai.partial_result" --\x3e "ai.speech_detect": "Speech finalized"'}),"\n",(0,i.jsxs)(t.p,{children:["note: Every partial result ",(0,i.jsx)(t.strong,{children:"replaces"})," last partial result unless a ",(0,i.jsx)(t.code,{children:"speech_detect"})," event is sent."]}),"\n",(0,i.jsx)(t.h2,{id:"when-ai-is-talking-uninterrupted",children:"When AI is talking uninterrupted:"}),"\n",(0,i.jsx)(t.mermaid,{value:'stateDiagram-v2\n    "ai.response_utterance" --\x3e "ai.response_utterance": "AI speaking"\n    "ai.response_utterance" --\x3e "ai.completion": "AI Is done talking (TTS Done)"'}),"\n",(0,i.jsxs)(t.p,{children:["note: Every response utterance ",(0,i.jsx)(t.strong,{children:"adds up"})," to the final complete response."]}),"\n",(0,i.jsx)(t.h2,{id:"when-user-interrupts-barge",children:"When user interrupts ('barge'):"}),"\n",(0,i.jsx)(t.mermaid,{value:"stateDiagram-v2\n    ai.response_utterance --\x3e ai.partial_result,barged=true\n    ai.partial_result,barged=true --\x3e ai.completion,type=barged"}),"\n",(0,i.jsx)(t.h2,{id:"handling-in-code",children:"handling in code"}),"\n",(0,i.jsx)(t.mermaid,{value:"stateDiagram-v2\n    [*] --\x3e ai.response_utterance\n    ai.response_utterance --\x3e ai.completion:uninterrupted ai tts fin\n\n    ai.response_utterance --\x3e ai.partial_result : barged=true\n    ai.partial_result --\x3e ai.completion : type=barged\n\n    ai.partial_result --\x3e ai.speech_detect"}),"\n",(0,i.jsx)(t.h2,{id:"deciding-who-spoke-last",children:"Deciding who spoke last"}),"\n",(0,i.jsx)(t.p,{children:"It looks necessary to keep track of partial text from both AI and user as conversation progresses. But\nto decide who spoke last, we use the following heuristic:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"if lastevent is ai.response_utterance, then AI spoke last\nif lastevent is ai.completion type=normal, then AI spoke last\nif lastevent is ai.completion type=barged, then user spoke last\n\nif lastevent is ai.partial_result barged=true, then user spoke last and this is the new beginning of user speech\nif lastevent is ai.speech_detect, then user spoke last\n"})}),"\n",(0,i.jsx)(t.h1,{id:"deciding-when-things-go-to-history",children:"Deciding when things go to history"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"if lastevent is ai.response_utterance, then add to partial result; last_spoken=ai\nif lastevent is ai.completion type=normal, then push to history; reset ai partial result; last_spoken=ai\nif lastevent is ai.completion type=barged, then push to history; reset ai partial result; last_spoken=user\n\nif lastevent is ai.partial_result barged=true, then replace user partial result; last_spoken=user\nif lastevent is ai.partial_result barged=false, then replace user partial result; last_spoken=user\nif lastevent is ai.speech_detect, then add to history; reset user partial result; last_spoken=user\n"})}),"\n",(0,i.jsx)(t.h2,{id:"code-description",children:"Code description:"}),"\n",(0,i.jsxs)(t.p,{children:["The ",(0,i.jsx)(t.code,{children:"Call"})," class manages the ",(0,i.jsx)(t.code,{children:"SignalWireClient"})," and the ",(0,i.jsx)(t.code,{children:"Call"})," object.\nSo for now, it is a reasonable place to hook into the ",(0,i.jsx)(t.code,{children:"SignalWireClient"})," chat events."]}),"\n",(0,i.jsxs)(t.p,{children:["The ",(0,i.jsx)(t.code,{children:"Chat"})," class manages the chat state. The Chat UI is rendered by ",(0,i.jsx)(t.code,{children:"chat.ui.ts"})]}),"\n",(0,i.jsx)(t.p,{children:"To render chat UI, I'm simply transforming all chat history into DOM with a pure function."})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);